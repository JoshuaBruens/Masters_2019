{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Extract hyperlinks from descriptions\n",
    "# Add them to another column\n",
    "# Look at descriptions and see if people identify as Dem or Rep\n",
    "# Male and Female\n",
    "# Extract common jobs and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('reduced_twitter_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdb = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdb['id'] =    list(map(lambda tweet_user: tweet_user['id'], df['user']))\n",
    "userdb['name'] =    list(map(lambda tweet_user: tweet_user['name'], df['user']))\n",
    "userdb['location'] =    list(map(lambda tweet_user: tweet_user['location'], df['user']))\n",
    "userdb['description'] =    list(map(lambda tweet_user: tweet_user['description'], df['user']))\n",
    "userdb['verified'] =    list(map(lambda tweet_user: tweet_user['verified'], df['user']))\n",
    "userdb['created_at'] =    list(map(lambda tweet_user: tweet_user['created_at'], df['user']))\n",
    "userdb['lang'] =    list(map(lambda tweet_user: tweet_user['lang'], df['user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A function that extracts the hyperlinks from the tweet's content.\n",
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "def remove_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    new_text = re.sub(regex,'',text)\n",
    "    return new_text\n",
    "\n",
    "def replace_word(text,word):\n",
    "    new_text = text\n",
    "    for loop in word:\n",
    "        new_text = re.sub(loop,' ',new_text)\n",
    "    return new_text\n",
    "\n",
    "# A function that checks whether a word is included in the tweet's content\n",
    "def word_in_text(word, text):\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def lowercase (doc):\n",
    "    doc=doc.lower()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this simple!!!\n",
    "temp = []\n",
    "temp_1 = []\n",
    "for loop in range(len(userdb['description'])):\n",
    "    if isinstance(userdb['description'][loop],str):\n",
    "        temp.append(extract_link(userdb['description'][loop]))\n",
    "        temp_1.append(remove_link(userdb['description'][loop]))\n",
    "    else:\n",
    "        temp.append('')\n",
    "        temp_1.append('')\n",
    "        \n",
    "\n",
    "\n",
    "userdb['link'] = temp        \n",
    "userdb['description'] = temp_1  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = []\n",
    "for loop in range(len(userdb['description'])):\n",
    "    if isinstance(userdb['description'][loop],str):\n",
    "        temp_2.append(   lowercase(userdb['description'][loop])     )\n",
    "    else:\n",
    "        temp_2.append('')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdb['description'] = temp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93282"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdb_reduced = userdb.drop_duplicates()\n",
    "len(userdb_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdb_train = userdb_reduced\n",
    "userdb_tweek = userdb_reduced[74638:83968]\n",
    "userdb_test = userdb_reduced[83968:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "exclude.remove('#')\n",
    "stop.update(['im','like','de','la','en','que','el','e','mi','del','un',\"i'm\",'los','sc','u','le','here','you','19','20','la','por','one','new','alum','tweet','own','former'])\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "clean_1 = []\n",
    "\n",
    "for loop in range(len(userdb_train)):\n",
    "    clean_1.append(clean(userdb_train.iloc[loop]['description']))    \n",
    "\n",
    "userdb_train.loc[:,'description'] = clean_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in userdb_train[userdb_train['lang'] == 'en']['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awscwi', 'pipefitter', 'usaf', 'nra', 'remember', 'usa', 'hunting', 'fishing', 'harleydavidsons', 'time', 'h', 'student', 'could', 'carry', 'gun', 'trks', '2', 'school']\n",
      "could\n",
      "[(13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)]\n"
     ]
    }
   ],
   "source": [
    "print (doc_clean[1])\n",
    "print (dictionary[16])\n",
    "print (doc_term_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=8, id2word = dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 -> (0, '0.045*\"year\" + 0.028*\"old\" + 0.021*\"hard\" + 0.018*\"fan\" + 0.016*\"yr\" + 0.015*\"working\" + 0.013*\"w\" + 0.010*\"die\"')\n",
      "Topic 1 -> (1, '0.034*\"thing\" + 0.032*\"right\" + 0.028*\"human\" + 0.026*\"law\" + 0.024*\"better\" + 0.014*\"making\" + 0.013*\"place\" + 0.012*\"around\"')\n",
      "Topic 2 -> (2, '0.079*\"fan\" + 0.057*\"lover\" + 0.052*\"•\" + 0.034*\"enthusiast\" + 0.024*\"food\" + 0.021*\"animal\" + 0.019*\"sport\" + 0.016*\"geek\"')\n",
      "Topic 3 -> (3, '0.019*\"senior\" + 0.013*\"phd\" + 0.012*\"going\" + 0.010*\"relation\" + 0.009*\"snap\" + 0.009*\"believer\" + 0.009*\"assistant\" + 0.009*\"earth\"')\n",
      "Topic 4 -> (4, '0.033*\"since\" + 0.032*\"endorsement\" + 0.022*\"stuff\" + 0.021*\"rts\" + 0.019*\"executive\" + 0.019*\"state\" + 0.013*\"≠\" + 0.013*\"n\"')\n",
      "Topic 5 -> (5, '0.079*\"love\" + 0.047*\"music\" + 0.025*\"art\" + 0.022*\"jesus\" + 0.021*\"book\" + 0.020*\"travel\" + 0.017*\"nerd\" + 0.016*\"game\"')\n",
      "Topic 6 -> (6, '0.031*\"view\" + 0.029*\"opinion\" + 0.029*\"university\" + 0.017*\"student\" + 0.015*\"public\" + 0.014*\"education\" + 0.014*\"political\" + 0.012*\"rt\"')\n",
      "Topic 7 -> (7, '0.037*\"real\" + 0.035*\"guy\" + 0.020*\"fun\" + 0.016*\"mind\" + 0.016*\"estate\" + 0.014*\"give\" + 0.014*\"please\" + 0.011*\"video\"')\n",
      "Topic 8 -> (8, '0.047*\"god\" + 0.039*\"family\" + 0.035*\"go\" + 0.033*\"retired\" + 0.030*\"love\" + 0.021*\"country\" + 0.017*\"veteran\" + 0.015*\"usa\"')\n",
      "Topic 9 -> (9, '0.026*\"manager\" + 0.024*\"founder\" + 0.019*\"designer\" + 0.019*\"consultant\" + 0.018*\"entrepreneur\" + 0.016*\"ceo\" + 0.014*\"member\" + 0.013*\"marketing\"')\n",
      "Topic 10 -> (10, '0.047*\"american\" + 0.040*\"conservative\" + 0.021*\"christian\" + 0.020*\"pro\" + 0.017*\"official\" + 0.016*\"patriot\" + 0.014*\"truth\" + 0.013*\"retweets\"')\n",
      "Topic 11 -> (11, '0.043*\"always\" + 0.024*\"think\" + 0.022*\"coach\" + 0.021*\"sometimes\" + 0.015*\"baby\" + 0.013*\"mean\" + 0.012*\"funny\" + 0.012*\"17\"')\n",
      "Topic 12 -> (12, '0.114*\"life\" + 0.042*\"live\" + 0.039*\"living\" + 0.034*\"time\" + 0.034*\"love\" + 0.023*\"day\" + 0.022*\"good\" + 0.018*\"best\"')\n",
      "Topic 13 -> (13, '0.063*\"news\" + 0.037*\"sport\" + 0.029*\"politics\" + 0.021*\"football\" + 0.017*\"junkie\" + 0.016*\"show\" + 0.016*\"political\" + 0.016*\"lot\"')\n",
      "Topic 14 -> (14, '0.032*\"twitter\" + 0.032*\"writer\" + 0.030*\"artist\" + 0.023*\"producer\" + 0.021*\"snapchat\" + 0.020*\"actor\" + 0.018*\"director\" + 0.013*\"national\"')\n",
      "Topic 15 -> (15, '0.078*\"husband\" + 0.072*\"father\" + 0.043*\"dad\" + 0.017*\"two\" + 0.017*\"boy\" + 0.015*\"tech\" + 0.014*\"trying\" + 0.013*\"well\"')\n",
      "Topic 16 -> (16, '0.052*\"business\" + 0.043*\"owner\" + 0.019*\"❤️\" + 0.015*\"care\" + 0.015*\"cant\" + 0.013*\"dc\" + 0.013*\"co\" + 0.012*\"kind\"')\n",
      "Topic 17 -> (17, '0.027*\"photographer\" + 0.022*\"change\" + 0.021*\"thought\" + 0.015*\"blogger\" + 0.015*\"writer\" + 0.014*\"freelance\" + 0.014*\"may\" + 0.014*\"creative\"')\n",
      "Topic 18 -> (18, '0.019*\"texas\" + 0.019*\"much\" + 0.019*\"golf\" + 0.014*\"order\" + 0.014*\"bad\" + 0.013*\"day\" + 0.012*\"radio\" + 0.012*\"soul\"')\n",
      "Topic 19 -> (19, '0.074*\"world\" + 0.020*\"science\" + 0.020*\"musician\" + 0.018*\"see\" + 0.017*\"culture\" + 0.014*\"nothing\" + 0.013*\"black\" + 0.013*\"formerly\"')\n",
      "Topic 20 -> (20, '0.046*\"get\" + 0.031*\"need\" + 0.019*\"help\" + 0.018*\"18\" + 0.015*\"write\" + 0.015*\"learning\" + 0.012*\"keep\" + 0.010*\"100\"')\n",
      "Topic 21 -> (21, '0.037*\"man\" + 0.021*\"rock\" + 0.020*\"hate\" + 0.019*\"player\" + 0.014*\"4\" + 0.013*\"lady\" + 0.013*\"gay\" + 0.013*\"star\"')\n",
      "Topic 22 -> (22, '0.044*\"born\" + 0.019*\"raised\" + 0.019*\"fanatic\" + 0.018*\"feminist\" + 0.016*\"currently\" + 0.014*\"amateur\" + 0.013*\"true\" + 0.012*\"shit\"')\n",
      "Topic 23 -> (23, '0.044*\"trump\" + 0.036*\"follow\" + 0.025*\"make\" + 0.023*\"supporter\" + 0.020*\"back\" + 0.020*\"america\" + 0.018*\"2016\" + 0.016*\"support\"')\n",
      "Topic 24 -> (24, '0.030*\"mine\" + 0.023*\"big\" + 0.021*\"future\" + 0.021*\"personal\" + 0.019*\"person\" + 0.017*\"find\" + 0.014*\"party\" + 0.012*\"opinion\"')\n",
      "Topic 25 -> (25, '0.034*\"#imwithher\" + 0.014*\"#blacklivesmatter\" + 0.013*\"ohio\" + 0.011*\"blocked\" + 0.010*\"liberal\" + 0.010*\"rip\" + 0.010*\"ive\" + 0.010*\"put\"')\n",
      "Topic 26 -> (26, '0.043*\"social\" + 0.036*\"medium\" + 0.022*\"know\" + 0.020*\"say\" + 0.017*\"health\" + 0.016*\"technology\" + 0.016*\"want\" + 0.014*\"digital\"')\n",
      "Topic 27 -> (27, '0.032*\"editor\" + 0.028*\"journalist\" + 0.019*\"reporter\" + 0.017*\"got\" + 0.016*\"city\" + 0.015*\"story\" + 0.013*\"part\" + 0.012*\"group\"')\n",
      "Topic 28 -> (28, '0.040*\"wife\" + 0.037*\"mom\" + 0.027*\"mother\" + 0.025*\"2\" + 0.023*\"teacher\" + 0.021*\"love\" + 0.021*\"kid\" + 0.021*\"proud\"')\n",
      "Topic 29 -> (29, '0.021*\"instagram\" + 0.020*\"service\" + 0.020*\"first\" + 0.014*\"development\" + 0.011*\"job\" + 0.011*\"25\" + 0.010*\"texan\" + 0.010*\"#trump2016\"')\n"
     ]
    }
   ],
   "source": [
    "# Print 1: Looked at all the english tweets and split it up into 4 topics??\n",
    "topics = ldamodel.print_topics(num_topics=8, num_words=8)\n",
    "# coherence score \n",
    "i=0\n",
    "for topic in topics:\n",
    "    print (\"Topic\",i ,\"->\", topic)     \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(words):\n",
    "    if words != '+':\n",
    "        num_1 = words.find('*')\n",
    "        num = int((float(words[0:num_1]) * 1000)/2)\n",
    "        num_1 = words.find('\"')\n",
    "        word = words[num_1+1:-1]\n",
    "        return num, word\n",
    "    return 0,''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[],[],[],[],[],[],[],[]]\n",
    "for loop in range(8):\n",
    "    res[loop] = topics[loop][1].split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[],[],[],[],[],[],[],[]]\n",
    "res[0] =  '0.022*today\" + 0.019*\"great\" + 0.013*\"pressure\" + 0.013*\"rain\" + 0.013*\"forecast\" + 0.013*\"orchard\" + 0.013*\"tempcrab\" + 0.012*\"0in\" + 0.011*\"fine\" + 0.010*\"american\"'\n",
    "res[1] = '0.022*\"tax\" + 0.015*\"amp\" + 0.010*\"god\" + 0.008*\"release\" + 0.007*\"time\" + 0.007*\"return\" + 0.007*\"idiot\" + 0.006*\"money\" + 0.006*\"them\" + 0.005*\"email\"'\n",
    "res[2] ='0.010*\"plan\" + 0.010*\"question\" + 0.008*\"hate\" + 0.008*\"trump\" + 0.008*\"press\" + 0.007*\"me\" + 0.006*\"sad\" + 0.006*\"yes\" + 0.006*\"fuck\" + 0.006*\"answer\"'\n",
    "res[3] = '0.028*\"\" + 0.008*\"president\" + 0.006*\"open\" + 0.006*\"evil\" + 0.005*\"problem\" + 0.005*\"honest\" + 0.004*\"border\" + 0.004*\"happy\" + 0.004*\"killed\" + 0.004*\"ha\"'\n",
    "res[4] = '0.019*\"it\" + 0.013*\"true\" + 0.010*\"talk\" + 0.007*\"usa\" + 0.007*\"wow\" + 0.007*\"russia\" + 0.006*\"friend\" + 0.006*\"war\" + 0.005*\"public\" + 0.004*\"idea\"'\n",
    "res[5] = '0.027*\"hillary\" + 0.023*\"amp\" + 0.018*\"clinton\" + 0.018*\"vote\" + 0.011*\"good\" + 0.008*\"him\" + 0.008*\"obama\" + 0.007*\"remember\" + 0.006*\"elected\" + 0.006*\"foundation\"'\n",
    "res[6] ='0.050*\"trump\" + 0.018*\"amp\" + 0.015*\"people\" + 0.011*\"vote\" + 0.011*\"donald\" + 0.010*\"hillary\" + 0.010*\"love\" + 0.010*\"campaign\" + 0.010*\"america\" + 0.009*\"racist\"'\n",
    "res[7] ='0.016*\"trump\" +0.012*\"wall\" + 0.011*\"pay\" + 0.010*\"liar\" + 0.010*\"work\" + 0.009*\"amp\" + 0.008*\"big\" + 0.007*\"day\" + 0.006*\"it\" + 0.006*\"mexico\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loop in range(8):\n",
    "    res[loop] = res[loop].split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_string = ['','','','','','','','']\n",
    "i = 0\n",
    "for topics_string in (res):\n",
    "    for words_words in (topics_string):\n",
    "        num , word = extract(words_words)\n",
    "        temp = ''\n",
    "        for loop in range(num):\n",
    "               temp = temp + ' ' + word\n",
    "        topic_string[i] =  topic_string[i] + ' ' + temp\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'               great great great great great great great great great   pressure pressure pressure pressure pressure pressure   rain rain rain rain rain rain   forecast forecast forecast forecast forecast forecast   orchard orchard orchard orchard orchard orchard   tempcrab tempcrab tempcrab tempcrab tempcrab tempcrab   0in 0in 0in 0in 0in 0in   fine fine fine fine fine   american american american american american'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-42ec367e8a33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_string' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400).generate(topic_string[0])\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
